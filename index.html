<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We present VISTA, a training approach for VLA models that strengthens visual grounding and improves task performance.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="visual grounding, visual conditioning, VLA, vision-language-action models, vision-language models, VLMs, Direct Preference Optimization, DPO, visual tracks, tracking, foundation models, robot learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Yiye Chen, Yanan Jian, Xiaoyi Dong, Shuxin Cao, Jing Wu, Patricio Vela, Benjamin E. Lundell, Dongdong Chen">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Yiye Chen">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We present VISTA, a training approach for VLA models that strengthens visual grounding and improves task performance.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://VISTA-VLA.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <!-- <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/VISTA_preview_1200x630.png"> -->
  <meta property="og:image" content="https://VISTA-VLA.github.io/static/images/VISTA_preview_1200x630.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="VISTA - Research Preview">
  <!-- <meta property="article:published_time" content="2024-01-01T00:00:00.000Z"> -->
  <meta property="article:author" content="Yiye Chen">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Vision-Language-Action Models">
  <meta property="article:tag" content="Direct Preference Optimization">
  <meta property="article:tag" content="Visual Tracks">
  <meta property="article:tag" content="Visual Conditioning">
  <meta property="article:tag" content="Visual Grounding">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <!-- <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE"> -->
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@chen_yiye">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We present VISTA, a training approach for VLA models that strengthens visual grounding and improves task performance.">
  <!-- TODO: Same as social preview image above -->
  <!-- <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png"> -->
  <meta name="twitter:image" content="https://VISTA-VLA.github.io/static/images/VISTA_preview_1200x630.png">
  <meta name="twitter:image:alt" content="VISTA - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models">
  <meta name="citation_author" content="Chen, Yiye">
  <meta name="citation_author" content="Jian, Yanan">
  <meta name="citation_author" content="Xiaoyi, Dong">
  <meta name="citation_author" content="Shuxin, Cao">
  <meta name="citation_author" content="Wu, Jing">
  <meta name="citation_author" content="Vela, Patricio">
  <meta name="citation_author" content="Lundell, Benjamin E.">
  <meta name="citation_author" content="Chen, Dongdong">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="arXiv">
  <!-- ######################################### TODO TODO: -->
  <!-- <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf"> -->
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
   <!-- NOTE: this will display on the web tab-->
  <title>VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models - Yiye Chen | Academic Research</title>   
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/robot_eye_icon_simple_white.png">
  <link rel="apple-touch-icon" href="static/images/robot_eye_icon_simple_white.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <!-- TODO TODO: URLs and Citations below -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
    "description": "We present VISTA, a training approach for VLA models that strengthens visual grounding and improves task performance.",
    "author": [
      {
        "@type": "Person",
        "name": "Yiye Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "IVALab, Georgia Tech"
        }
      },
      {
        "@type": "Person",
        "name": "Yanan Jian",
        "affiliation": {
          "@type": "Organization",
          "name": "NVIDIA"
        }
      }
      {
        "@type": "Person",
        "name": "Xiaoyi Dong",
        "affiliation": {
          "@type": "Organization",
          "name": "Microsoft"
        }
      }
      {
        "@type": "Person",
        "name": "Shuxin Cao",
        "affiliation": {
          "@type": "Organization",
          "name": "Georgia Tech"
        }
      }
      {
        "@type": "Person",
        "name": "Jing Wu",
        "affiliation": {
          "@type": "Organization",
          "name": "Oxford"
        }
      }
      {
        "@type": "Person",
        "name": "Patricio Vela",
        "affiliation": {
          "@type": "Organization",
          "name": "Georgia Tech"
        }
      }
      {
        "@type": "Person",
        "name": "Benjamin E. Lundell",
        "affiliation": {
          "@type": "Organization",
          "name": "ARM"
        }
      }
      {
        "@type": "Person",
        "name": "Dongdong Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "Microsoft"
        }
      }
    ],
    "datePublished": "2026-01",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "static/images/VISTA_preview_1200x630.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Robot Learning"
      },
      {
        "@type": "Thing", 
        "name": "Machine Learning"
      },
      {
        "@type": "Thing", 
        "name": "Vision-Language-Action Models"
      },
      {
        "@type": "Thing", 
        "name": "Direct Preference Optimization"
      },
      {
        "@type": "Thing", 
        "name": "Foundation Models"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "IVALab, Georgia Tech",
    "url": "https://yiyechen.github.io/",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/chen_yiye",
      "https://github.com/yiyeChen"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list"> -->
        <!-- TODO: Replace with your lab's related works -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info"> -->
            <!-- TODO: Replace with actual paper title -->
            <!-- <h5>Paper Title 1</h5> -->
            <!-- TODO: Replace with brief description -->
            <!-- <p>Brief description of the work and its main contribution.</p> -->
            <!-- TODO: Replace with venue and year -->
            <!-- <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a> -->
        <!-- TODO: Add more related works or remove extra items -->
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <!-- <div class="container is-max-fullhd"> -->
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in VLAs</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://yiyechen.github.io/" target="_blank">Yiye Chen</a><sup>1†</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=K0ljSw4AAAAJ&hl=en" target="_blank">Yanan Jian</a><sup>2‡</sup>,</span>
                  <span class="author-block">
                    <a href="https://lightdxy.github.io/" target="_blank">Xiaoyi Dong</a><sup>3</sup>,</span>
                    <span class="author-block">
                      <a href="https://www.linkedin.com/in/shuxin-cao-4827401aa/?originalSubdomain=sg" target="_blank">Shuxin Cao</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="https://jingwu2121.github.io/" target="_blank">Jing Wu</a><sup>4†</sup>,</span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=qL6ycTgAAAAJ&hl=en" target="_blank">Patricio Vela</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="https://www.linkedin.com/in/benjamin-lundell/" target="_blank">Benjamin E. Lundell</a><sup>5‡</sup>,</span>
                            <span class="author-block">
                              <a href="https://www.dongdongchen.bid/" target="_blank">Dongdong Chen</a><sup>3</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <!-- <span class="author-block">Institution Name<br>Conference name and year</span> -->
                    <span class="author-block"><sup>1</sup>Georgia Institute of Technology</span>
                    <span class="author-block"><sup>2</sup>Nvidia</span>
                    <span class="author-block"><sup>3</sup>Microsoft GenAI</span>
                    <span class="author-block"><sup>4</sup>University of Oxford</span>
                    <span class="author-block"><sup>5</sup>ARM</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    <span class="eql-cntrb"><br><sup>†</sup>Work done during an internship at Microsoft.</span>
                    <span class="eql-cntrb"><br><sup>‡</sup>Work done while working at Microsoft.</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2602.05049.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Release soon!)</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2602.05049" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
      <!-- </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser (Left text, Right image) -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <div class="has-text-centered">
        <h2 class="title is-3">Overview</h2>
        <div class="title-underline"></div>
      </div>

      <div class="columns is-vcentered is-variable is-8">

        <!-- Left: Text -->
        <div class="column is-7-desktop is-12">
          <!-- <h2 class="title is-3 has-text-centered">Overview</h2> -->

          <!-- <p class="content is-size-5">
            Write 2–4 sentences that summarize your paper: what problem you solve,
            your key idea, and the headline result/benefit.
          </p> -->

          <ol class="content is-size-5">
            
            <li><strong>Key idea:</strong> We study and demonstrate the importance of <span class="highlight-red">visual conditioning for VLA</span> performance.</li>
            <li><strong>Key method:</strong> We introduce VISTA, a <span class="highlight-red">new training framework</span> that improves visual conditioning in VLAs by <span class="underline"><em>aligning action predictions to visual tracks</em></span>.</li>
            <li><strong>Major Results:</strong> VISTA <span class="highlight-red">enhances visual conditioning</span> of OpenVLA, and <span class="highlight-red">improves performance</span> of both OpenVLA and OpenVLA-OFT.</li>
          </ol>
        </div>

        <!-- Right: Image -->
        <div class="column is-5-desktop is-12">
          <figure class="teaser-figure">
            <img
              src="static/images/teaser-VISTA_crop.png"
              alt="Teaser figure"
              class="teaser-image"
            />
            <figcaption class="has-text-grey is-size-6">
              <!-- Figure: Short caption describing what the image shows. -->
              <strong>VISTA Overview.</strong> 
            </figcaption>
          </figure>
        </div>

      </div>

    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Vision–Language–Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks.
            Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs.
            In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones.
            Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models.
            Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, 
            and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning.
            Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/VCKL_BaseTFOurs_libSpa.png" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->


<!-- Single Image. No carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
       <h2 class="title is-3 has-text-centered">Visual Conditioning Study and Results</h2>
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/VCKL_BaseTFOurs_libSpa.png" alt="Visual conditioning result visualization" loading="lazy" 
             style="width: 75%; height: auto; display: block; margin: 0 auto;"/>
        <!-- TODO: Replace with description of this result -->
        <!-- <div class="column is-four-fifths"> -->
          <!-- <h2 class="subtitle has-text-centered">
            <div class="content has-text-justified">
              Token-level visual conditioning of 8-step OpenVLA and VISTA (ours) in LIBERO-Spatial
            </div>
          </h2> -->
        <!-- </div> -->
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <p class="has-text-centered is-size-6">
                <em>
                  Token-level visual conditioning of 8-step OpenVLA and VISTA (<strong>Ours</strong>) in LIBERO-Spatial. <br>
                  The vertical grids indicate that every 7 tokens decode to 1 action (56 tokens for 8 actions in total).
                </em>
              </p>
            </div>
          </div>
        <ul class="content is-size-5" style="list-style-type: disc;">
          
          <li>We <strong>quantify VLA token-level visual conditioning</strong> as KL divergence between the action distributions conditioned on clean and perturbed visual inputs.</li>
          <li>We study <strong>baseline autoregressive OpenVLA</strong> and show that <span class="highlight-red">failed rollouts</span> <strong>feature consistently weaker visual conditioning</strong> than <span class="highlight-green">successful rollouts</span>. </li>
          <li><span class="highlight-purple">Our method (VISTA)</span><strong> enhances visual conditioning and improves task performance</strong> (see below) </li>
        </ul>
      </div>
</div>
</div>
</section>
<!-- End single image no carousel -->


<!-- Single Image. No carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
       <h2 class="title is-3 has-text-centered">Method</h2>
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/VISTA_method_1200x630.jpg" alt="Method" loading="lazy" 
             style="width: 90%; height: auto; display: block; margin: 0 auto;"/>
        <!-- TODO: Replace with description of this result -->
        <!-- <div class="column is-four-fifths"> -->
          <!-- <h2 class="subtitle has-text-centered">
            <div class="content has-text-justified">
              Token-level visual conditioning of 8-step OpenVLA and VISTA (ours) in LIBERO-Spatial
            </div>
          </h2> -->
        <!-- </div> -->
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <p class="has-text-centered is-size-6">
                <em>
                  Illustration of VISTA training recipe.
                </em>
              </p>
            </div>
          </div>
        <div class="content is-size-5">
          VISTA is a training framework consisting of 3 stages:
        </div>
        <ol class="content is-size-5" start="0">
          
          <li>Regular instruction-following Supervised finetuning (SFT). </li>
          <li>Track-following Direct Preference Optimization (DPO) for aligning vision and action. </li>
          <li>Instruction-following SFT with latent distillation (cosine similary by default) from the aligned model. </li>
        </ol>
      </div>
</div>
</div>
</section>
<!-- End single image no carousel -->




<!-- LIBERO results and videos -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- title. -->
      <h2 class="title is-3 has-text-centered">LIBERO Experimental Results and Demos</h2>
      <!-- Result table -->
      <div class="content is-size-5">
        <strong>LIBERO experimental results:</strong>
      </div>
      <div class="content">
        <img src="static/images/LIBERO_results.png" alt="LIBERO_results" loading="lazy" 
           style="width: 50%; height: auto; display: block; margin: 0 auto;"/>
      </div>
      <!-- Demo videos. [PROBLEM] If I put only video here, the rendering of the gifs in the CALVIN section will be wrong. It will display all gifs at the same time in a column. -->
      <div class="content is-size-5">
        <strong>LIBERO demo videos (discrete VISTA):</strong>
      </div>

      <div id="libero-results-carousel" class="carousel results-carousel" data-slides-to-show="3">
        <!-- libero Long  -->
        <div class="item item-video7">
          <video poster="" id="video7" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/long1.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Long 1] pick up the bbq sauce and place it in the basket</p>
        </div>
        <div class="item item-video8">
          <video poster="" id="video8" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/long2.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Long 2] put both moka pots on the stove </p>
        </div>
        <!-- libero spatial -->
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/spatial1.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Spatial 1] pick up the black bowl on the ramekin and place it on the plate </p>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/spatial2.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Spatial 2] pick up the black bowl on the wooden cabinet and place it on the plate </p>
        </div>
        <!-- libero goal -->
        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/goal1.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Goal 1] push the plate to the front of the stove</p>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/goal2.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Goal 2] open the middle drawer of the cabinet </p>
        </div>
        <!-- libero object -->
        <div class="item item-video5">
          <video poster="" id="video5" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/object1.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Object 1] pick up the bbq sauce and place it in the basket</p>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" controls muted loop height="100%" preload="metadata"
            onloadedmetadata="this.playbackRate = 0.4;"
          >
            <source src="static/videos/object2.mp4" type="video/mp4">
          </video>
          <p class="demo-vid-caption">[Object 2] pick up the ketchup and place it in the basket</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End LIBERO results and videos -->


<!-- CALVIN results and videos -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- title. -->
      <h2 class="title is-3 has-text-centered">CALVIN Experimental Results and Demos</h2>
      <!-- Result table -->
      <div class="content is-size-5">
        <strong>CALVIN experimental results:</strong>
      </div>
      <div class="content">
        <img src="static/images/CALVIN_results.png" alt="LIBERO_results" loading="lazy" 
           style="width: 75%; height: auto; display: block; margin: 0 auto;"/>
      </div>
      <!-- Demo videos. TODO: Here is where I want to put a multi-video carousel -->
      <div class="content is-size-5">
        <strong>CALVIN demo videos (continuous VISTA-OFT):</strong>
      </div>
      <div id="calvin-results-carousel" class="carousel results-carousel" data-slides-to-show="3">
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_02_timeconcat.gif" alt="Demo 1" />
            <p class="demo-gif-caption">Demo 1</p>
          </div>
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_04_timeconcat.gif" alt="Demo 2" />
            <p class="demo-gif-caption">Demo 2</p>
          </div>
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_07_timeconcat.gif" alt="Demo 3" />
            <p class="demo-gif-caption">Demo 3</p>
          </div>
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_09_timeconcat.gif" alt="Demo 4" />
            <p class="demo-gif-caption">Demo 4</p>
          </div>
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_14_timeconcat.gif" alt="Demo 5" />
            <p class="demo-gif-caption">Demo 5</p>
          </div>
          <div class="item">
            <img class="demo-gif" src="static/videos/seq_16_timeconcat.gif" alt="Demo 6" />
            <p class="demo-gif-caption">Demo 6</p>
          </div>
      </div>

    </div>
  </div>
</section>
<!-- End CALVIN results and videos -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- Single Image. No carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
       <h2 class="title is-3 has-text-centered">Analysis</h2>
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/VC_training_dynamics.png" alt="Method" loading="lazy" 
             style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
        <!-- TODO: Replace with description of this result -->
        <!-- <div class="column is-four-fifths"> -->
          <!-- <h2 class="subtitle has-text-centered">
            <div class="content has-text-justified">
              Token-level visual conditioning of 8-step OpenVLA and VISTA (ours) in LIBERO-Spatial
            </div>
          </h2> -->
        <!-- </div> -->
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <p class="has-text-centered is-size-6">
                <em>
                  Visual Conditioning Across VISTA Training Stages.
                </em>
              </p>
            </div>
          </div>
        <div class="content is-size-5">
          Stage 1 with DPO significantly improves the visual conditioning in the track-following task. <br>
          Stage 2 with distillation transfers the enhanced visual conditioning to the instruction-following task compared with pure SFT in Stage 0.
        </div>
      </div>
</div>
</div>
</section>
<!-- End single image no carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{chen2026vista,
  title={VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models}, 
  author={Chen, Yiye and Jian, Yanan and Dong, Xiaoyi and Cao, Shuxin and Wu, Jing and Vela, Patricio and Lundell, Benjamin E. and Chen, Dongdong},
  journal={arXiv preprint arXiv:2602.05049},
  year={2026},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
